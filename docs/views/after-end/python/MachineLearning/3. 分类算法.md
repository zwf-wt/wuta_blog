# 分类算法
## 转换器 - 特征工程的父类
想一下之前做的特征工程的步骤？
1. 实例化(实例化的是一个转换器类(Transformer))
2. 调用 fit_transform (对于文档建立分类的词频矩阵，不能同时调用)

我们把特征工程的接口称之为转换器，其中转换器调用有这么几种形式
- fit_transform
- fit(X) 返回self
- transform(X)
## 估计器(sklearn 机器学习算法的实现)
在sklean中，估计器(estimator)是一个重要的概念。是一类实现了算法的API
1. 用于分类的估计器
  - sklearn.neighbors k-近邻算法
  - sklearn.naive_bayes 贝叶斯
  - sklearn.linear_model.LogisticRegression 逻辑回归
  - sklearn.tree 决策树与随机森林
2. 用于回归的估计器
  - sklearn.linear_model.LinearRegression 线性回归
  - sklearn.linear_model.Ridge 岭回归

3. 用于无监督学习的估计器
  - sklearn.cluster.KMeans 聚类

### 预估器工作流程
1. 训练集(x_train, y_train) - x_train: 训练集的特征值，y_train: 训练集的目标值
2. 实例化一个 estimator
3. estimator.fit(x_train, y_train) 计算
  ——调用完毕，模型生成
4. 模型评估：
  1) 直接比对真实值和预测值
      y_predict = estimator.predict(x_test)
      y_test == y_predict
  2) 计算准确率
      accuracy = estimator.score(x_test, y_test) - x_test: 测试集的特征值，y_test: 测试集的目标值
  
## K-近邻算法(根据你的邻居来推断出你的类别)
### K-近邻算法(KNN)算法原理
K Nearest Neighbor 算法又叫KNN算法，这个算法是机器学习里面一个比较经典的算法，总体来说KNN算法是相对比较容易理解的算法
#### 定义
如果一个枕头套在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别

#### 距离公式
两个样本的距离可以通过如下公式计算，又叫欧式距离。比如说，a(a1, a2, a3), b(b1, b2, b3)
sqrt((a1 - b1) ^ 2 + (a2 - b2) ^ 2 + (a3 - b3) ^ 2)

曼哈顿距离(绝对值距离)

明可夫斯基距离 

#### 电影类型分析
假设有一批电影，每部电影都有三个特征，分别是动作，冒险，爱情，根据这些特征，我们可以把电影分成不同的类型，比如动作片，冒险片，爱情片

|电影名称|打斗镜头|接吻镜头|电影类型|
|-------|--------|--------|--------|
|california Man|2|104|爱情片|
|He's notReally into dues'|2|100|爱情片|
|Beautiful Woman|1|81|爱情片|
|Kevin Longbiads|101|10|动作片|
|Robo Siayer 30000|99|5|动作片|
|Amped ll|98|2|动作片|
| ? |18|90|未知|
其中 ? 号电影不知道类别，如何去预测？可以利用 K 近邻算法的思想
|电影名称|与未知电影的距离|
|-------|---------------|
|california Man|20|
|He's notReally into dues'|18.7|
|Beautiful Woman|19.2|
|Kevin Longbiads|115.3|
|Robo Siayer 30000|117.4|
|Amped ll|118.9|

##### 问题
- 如果取的最近的电影数量不一样？会是什么结果？
  - k = 1, 爱情片。
  - k = 2 爱情片
  - k = 6 无法确定
  - k = 7 动作片
  - k 值取得过大，样本不均衡的影响
  - k 值取得过小，容易受异常值影响
  
  需要进行无量纲化的处理
### K-近邻算法(KNN)API
```python
sklearn.neighbors.KNeighborsClassifier(n_neighbors = 5, algorithm = 'auto')
# n_neighbors: int, 可选(默认为5), k_neighbors 查询默认使用的邻居数
# algorithm: {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, 可选用于计算最近邻居的算法

# ‘ball_tree’：将会使用BalTree, 在球树中执行最近邻搜索
# ‘kd_tree’：将使用KDTree, 在kd树中执行最近邻搜索
# ‘auto’：将尝试要付传递给fit 方法的值来决定最合适的算法。如果数据集的维度小于20，则使用kd树。否则使用球树。
# ‘brute’：将使用暴力搜索
```
### 案例1：鸢尾花种类预测
#### 数据集介绍
iris 数据集是常用的分类实验数据集，由Fisher, 1936 收集整理。Iris也称鸢尾花卉数据集，是一类多重变量分析的数据集。关于数据集的具体介绍：
- 实例数据：150(三个类各有50个)
- 属性数量：4(数值型，数值型，帮助预测的属性和类)
- Attribute Information:
  - sepal length 萼片长度 (cm)
  - sepal width 萼片宽度 (cm)
  - petal length 花瓣长度 (cm)
  - petal width 花瓣宽度 (cm)
  - class:
    iris-Setosa 山鸢尾
    iris-Versicolour 变色鸢尾
    iris-Virginica 维吉尼亚鸢尾

#### 代码实现
```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier

def knn_iris():
  """
    用 KNN 算法对鸢尾花数据集进行分类
  """
  # 1 获取数据
  iris = load_iris()
  # 2 划分数据集
  #                                                   特征值，    目标值
  x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state = 6)
  # 3 特征工程：标准化
  transfer = StandardScaler()
  x_train = transfer.fit_transform(x_train)

  x_test = transfer.transform(x_test)
  # 4 KNN算法预估
  estimator = KNeighborsClassifier(n_neighbors=3)
  estimator.fit(x_train, y_train)
  # 5 模型评估
  # 方法1：直接比双真实值和预测值
  y_predict = estimator.predict(x_test)
  print("y_predict:\n", y_predict)
  print("直接比对真实值和预测值：\n", y_test == y_predict)

  # 方法2：计算准确率
  score = estimator.score(x_test, y_test)
  print("准确率为：\n", score)

  return None

if __name__ == "__main__":
  knn_iris()
```
### 总结
- 优点：简单，易于理解，易于实现，无需估计参数
- 缺点：
  - 懒惰算法，对测试样本分类时的计算量大，内存开销大
  - 必须指定K值，K值选择不当则分类精度不能保证
- 使用场景：小数据场景，几千~几成样本，具体场景具体业务去测试
#### 模型评估
##### 什么是交叉验证(cross validation)
交叉验证：将拿到的训练数据分为训练和验证集。将数据分成4份，其中一份作为验证集。然后经过4次()测试，每次都更换不同的验证集。即得到4组模型的结果，取平均值作为最终结果。又称4折交叉验证
- 训练集：训练集 + 验证集
- 测试集：测试集
1. 验证集 ------- 训练集  ------- 训练集  ------- 训练集  80%
1. 训练集 ------- 验证集  ------- 训练集  ------- 训练集  78%
1. 训练集 ------- 训练集  ------- 验证集  ------- 训练集  75%
1. 训练集 ------- 训练集  ------- 训练集  ------- 验证集  82%
##### 为什么需要交叉验证
为了让模型更加准确，减少过拟合

##### 超参数搜索 - 网格搜索(Grid Search)
通常情况下，有很多参数是需要手动指定的(如 k-近邻算法中的k值)，这种叫超参数。但是手动过程繁杂，所以需要对模型预设几种超参数组合。每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建立模型。
##### 模型选择与调优API
- sklearn.model_selection.GridSearchCV(estimator, param_grid=None, cv=None)
  - 对估计器的指定参数值进行详尽搜索
  - estimator: 估计器对象
  - param_grid: 估计器参数(dict){"n_neighbors": [1, 3, 5]}
  - cv: 指定几折交叉验证
  - fit(): 输入训练数据
  - score(): 准确率
  - 结果分析：
    - 最佳参数：best_params_
    - 最佳结果：best_score_
    - 最佳估计器：best_estimator_
    - 交叉验证结果：cv_results_
```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV

def knn_iris_gscv():
  """
    用 KNN 算法对鸢尾花数据集进行分类, 添加网格搜索和交叉验证
  """
  # 1 获取数据
  iris = load_iris()
  # 2 划分数据集
  #                                                   特征值，    目标值
  x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state = 6)
  # 3 特征工程：标准化
  transfer = StandardScaler()
  x_train = transfer.fit_transform(x_train)

  x_test = transfer.transform(x_test)
  # 4 KNN算法预估
  estimator = KNeighborsClassifier()

  # 加入网格搜索和交叉验证
  # 参数准备
  param_dict = {"n_neighbors": [1, 3, 5, 7, 9, 11]}
  estimator = GridSearchCV(estimator, param_grid = param_dict, cv=10)

  estimator.fit(x_train, y_train)

  # 5 模型评估
  # 方法1：直接比双真实值和预测值
  y_predict = estimator.predict(x_test)
  print("y_predict:\n", y_predict)
  print("直接比对真实值和预测值：\n", y_test == y_predict)

  # 方法2：计算准确率
  score = estimator.score(x_test, y_test)
  print("准确率为：\n", score)

  print('最佳参数: \n', estimator.best_params_)
  print('最佳结果: \n', estimator.best_score_)
  print('最佳预估器: \n', estimator.best_estimator_)
  print('交叉验证结果: \n', estimator.cv_results_)

  return None


if __name__ == "__main__":
  knn_iris_gscv()
```

### 案例：预测facebook签到位(kaggle下载就行)
根据用户的位置，准确性和时间戳预测用户正在查看的业务签到地点。
- train.csv, test.csv
  - row_id: id (特征值)
  - x y: 经纬度 (特征值)
  - accuracy: 准确度 (特征值)
  - time: 时间戳 (特征值)
  - place_id: 签到地点id (目标值)

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
import pandas as pd


def facebook_deom():
  # 1. 获取数据
  data = pd.read_csv("./FBlocation/train.csv")
  # 2. 数据处理
  # 2.1 缩小数据范围
  data = data.query("x < 2.5 & x > 2 & y < 1.5 & y > 1.0")
  # 2.2 处理时间戳
  time_value = pd.to_datetime(data["time"], unit = "s") # 时间戳转换为时间格式 yyyy-mm-dd hh:mm:ss
  date = pd.DatetimeIndex(time_value)
  data["day"] = date.day
  data['weekday'] = date.weekday
  data['hour'] = date.hour
  # 2.3 过滤签到次数少的地点
  place_count = data = data.groupby('place_id').count()['row_id']
  data_final = data[data['place_id'].isin(place_count[place_count > 3].index.values)]

  # 筛选特征值和目标值
  x = data.final[
    ['x', 'y', 'accuracy', 'day', 'weekday', 'hour']
  ] # 特征值
  y = data.final['place_id'] # 目标值

  # 数据集划分
  x_train, x_test, y_train, y_test = train_test_split(x, y)
  # 3. 特征工程：标准化
  transfer = StandardScaler()
  x_train = transfer.fit_transform(x_train)
  x_test = transfer.transform(x_test)

  # 4. KNN算法预估流程
  estimator = KNeighborsClassifier()

  # 5. 模型选择与调优
  param_dict = {"n_neighbors": [1, 3, 5, 7, 9, 11]}
  estimator = GridSearchCV(estimator, param_grid = param_dict, cv=3)
  estimator.fit(x_train, y_train)

  # 6. 模型评估
  # 方法1：直接比双真实值和预测值
  y_predict = estimator.predict(x_test)
  print("y_predict:\n", y_predict)
  print("直接比对真实值和预测值：\n", y_test == y_predict)

  # 方法2：计算准确率
  score = estimator.score(x_test, y_test)
  print("准确率为：\n", score)

  print('最佳参数: \n', estimator.best_params_)
  print('最佳结果: \n', estimator.best_score_)
  print('最佳预估器: \n', estimator.best_estimator_)
  print('交叉验证结果: \n', estimator.cv_results_)


```

## 朴素贝叶斯算法
朴素(特征与特征之间相互独立) + 贝叶斯

### 什么是朴素贝叶斯算法
### 概率基础
- 概率定义为一件事发生的可能性
 - 扔出一个硬币，结果头像朝上
- P(X): 取值在[0, 1]
#### 联合概率、条件概率与相互独立
- 联合概率：包含多个条件，且所有条件同时成立的概率
  - 记作：P(A, B)
  - 例如：P(程序员，匀称)，P(程序员, 超重|喜欢)
- 条件概率：就是事件A在另外一个事件B已经发生条件下的发生概率
  - 记作：P(A|B)
  - 例如：P(程序员|喜欢), P(程序员, 超重|喜欢)
- 相互独立：如果P(A, B) = P(A)P(B), 则称事件A与事件B相互独立。

#### 贝叶斯公式
- 贝叶斯公式：
  - P(C|W) = P(W|C)P(C) / P(W)
  - P(C): 每个文档类别的概率(某文档类别数/总文档数量)
  - P(W|C): 给定类别下特征(被预测文档中出现的词) 的概率
    - 计算方法：P(F1 | C) = NI / N(训练文档中去计算)
      - Ni为该F1词在C类别所有文档中出现 的次数
      - N为所属类别C下的文档所有词出现的次数和
  - P(F1, F2,...) 预测文档中每个词的概率 
> w为给定文件的特征值(频数统计，预测文档提供)，C为文档类别
### 应用场景：
- 文本分类(单词作为特征)
### 案例
|       | 文档ID | 文档中的词                          | 属于c = China 类|
|-------|------- |------------------------------------|----------------|
| 训练集 | 1     | Chinese Beijing Chinese             | Yes           |
| 训练集 | 2     | Chinese Chinese Shanghai            | Yes           |
| 训练集 | 3     | Chinese Macao                       | Yes           |
| 训练集 | 4     | Tokyo Japan Chinese                 | No            |
| 测试集 | 5     | Chinese Chinese chinese Tokyo Japan | ?             |
#### 拉普拉斯平滑系数
目的：防止计算出的分类概率为0
P(F1 | C) = Ni + a / (n + a* m)，a为指定的系数一般为1，m为训练文档中统计出的特征词个数
P(Chinese | C) = (5 * 1) / (8 + 1 * 6) = 6 / 14 = 3 / 7
P(Tokyo | C) = (0 + 1) / (8 + 1 * 6) = 1 / 14
P(Japan | C) = (0 + 1) / (8 + 1 * 6) = 1 / 14
#### API
sklearn.naive_bayes.MultinomialNB(alpha = 1.0)
  - 朴素贝叶斯分类
  - alpha: 拉普拉斯平滑系数
### 案例：20类新闻分类
```python
from sklearn.datasets import fetch_20newsgroups
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB

def nb_news():
  """
    用朴素贝叶斯算法对新闻进行分类
  """

  # 1. 获取数据
  fetch_20newsgroups(subset='all')

  # 2. 划分数据集
  x_train, x_test, y_train, y_test = train_data = train_test_split(newes.data, news.target)
  
  # 3. 特征工程：文本特征抽取
  transfer = TfidfVectorizer()
  X_train = transfer.fit_transform(x_train)
  X_test = transfer.transform(x_test)

  # 4. 朴素贝叶斯算法预估器流程
  estimator = MultinomialNB()
  estimator.fit(X_train, y_train)
  
  # 5. 模型评估
  # 方法1：直接比对真实值和预测值
  y_predict = estimator.predict(X_test)
  print("y_predict:\n", y_predict)
  print("直接比对真实值和预测值：\n", y_test == y_predict)

  # 方法2：计算准确率
  score = estimator.score(X_test, y_test)
  print("准确率：\n", score)
  
  return None


if __name__ == '__main__':
  nb_news()

```
### 总结
- 优点
  1. 朴素贝叶斯模型发源于古典数学理论有稳定的分类效率。
  2. 对缺失数据不太敏感，算法也比较简单，常用于文本分类。
  3. 分类准确度高，速度快。
- 缺点
  1. 由于使用了样本属性独立性的假设，所以如果特征属性有关联时其效果不好。



## 集成学习方法之随机森林
### 什么是集成学习方法
集成学习通过建立几个模型组合的来解决单一预测问题。它的工作原理是生成多个分类器/模型，各自独立地学习和作出预测。这些预测最后结合成组合预测，因此优于任何一个单分类的做出预测。
### 什么是随机森林
在机器学习中，随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。

### 随机森林原理过程
- 用N来表示训练用例(样本)的个数，M表示特征数目。
  - 一次随机选出一个样本，重复N次，(有可能出现重复的样本)
  - 随机去选出m个特征，m << M, 建立决策树。
- 采取bootstrap抽样
#### 为什么采用BootStrap抽样
- 为什么要随机抽样训练集？
  - 如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的。

- 为什么要有放回地抽样？
  - 如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是"有偏的"，都是绝对"片面的"(当然这样说可能不对)，也就是说每棵树训练出来老师有很大的差异的；而随机森林最后分类取决于多棵树(弱分类器)的投票表决。

### API
- class sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion='gini',
  max_depth=None, bootstrap=True, random_state=None, min_samples_split=2
)
  - 随机森林分类器
  - n_estimators: integer, optional(default=10) 森林里的树木数量120, 200, 300, 500, 800, 1200
  - criteria: string,可选(default=”gini”)分割特征的测量方法
  - max_depth: integer 或 None, 可选(默认 = None) 树的最大深度 5,8,15,25,30
  - max_features="auto", 每个决策树的最大特征数量
    - if "auto", then max_features=sqrt(n_features)
    - if "sqrt", then max_features=sqrt(n_features) (same as "auto")
    - if "log2", then max_features=log2(n_features)
    - if None, then max_features=n_features
  - bootstrap: boolean, optional(default = True) 是否在构建树时使用放加抽样
  - min_samples_split: 叶子节点所需的最小样本数
  - min_samples_leaf: 节点划分最少样本数
- 超参数：n_estimator, max_depth, min_samples_split, min_samples_leaf


### 案例

### 总结
- 在当前所有算法中，有极高的准确性