# 特征工程

## 1. 数据集

### 1.1 可用数据集
#### 1. [scikit-learn](http://scikit-learn.org/stable/datasets/index.html#datasets)

1. 数据量较小
2. 方便学习

#### 2. [UCI](http://archive.ics.uci.edu/ml)
1. 收录了360个数据集
2. 覆盖科学、生活、经济等领域
3. 数据量几十万
#### 3. [Kaggle](https://www.kaggle.com/datasets)
1. 大数据竞赛平台
2. 80万科学家
3. 真实数据
4. 数据量巨多
## 2. Scikit-learn 工具介绍
- Python 语言的机器学习工具
- Scikit-learn 包括许多知名的机器学习算法的实现
- Scikit-learn 文档完善、容易上手，丰富的API
### 2.1 安装
```python
pip install Scikit-learn==0.19.1
pip install sklearn==0.19.1
```
安装好之后可以通过以下命令查看是否安装成功
```python
import sklearn
# 安装 scikit-learn 需要Numpy, Scipy等库
```
### 2.2 Scikit-learn 包含的内容
- 分类、聚类、回归
- 特征工程
- 模型选择、调优

### 2.3 sklearn 数据集
#### 1. scikit-learn 数据集 API 介绍
- sklearn.datasets
  - 加载获取流行数据集
  - datasets.load_*()
    - 获取小规模数据集，数据包含在datasets里
  - datasets.fetch_*(data_home=None)
    - 获取大规模数据集，需要从网络上下载，函数的第一个参数是data_home, 表示数据集下载的目录，默认是~/scikit_learn_data/
#### 2. sklearn 小数据集
- sklearn.datasets.load_iris() 加载并返回鸢尾花数据集

|名称|数量 |
|----- |-------|
|类别|  3  |
|特征|  4  |
|样本数据| 150 |
|每个类别数量| 50 |

- sklearn.datasets.load_boston() 加载并返回波士顿房价数据集

|名称|数量 |
|----- |-------|
|目标类别|  5-50  |
|特征|  13  |
|样本数据| 508 |

#### 3. sklearn 大数据集
- sklearn.datasets.fetch_20newsgroups(data_home=None, subset='train')
  - subset: 'train' 或者 'test', 'all', 可选，选择要加载的数据集
  - 训练集的"训练"，测试集的"测试", 两者的"全部"

#### 4. sklearn数据集的使用
- 以鸢尾花数据集为例：
  - 特征值-4个：花瓣、花萼的长度、宽度
  - 目标值-3个：setosa, vericolor, virginica

##### sklearn 数据集返回值介绍
- load 和 fetch 返回的数据类型 datasets.base.Bunch(字典格式)
  - data: 特征数据数组，是[n_samples * n_features]的二维numpy.ndarray数组
  - target: 标签数组,是[n_samples]的一维numpy.ndarray数组
  - DESCR: 数据描述, 字典格式, 描述数据集的信息
  - feature_names: 特征名, 新闻数据，手写数字、回归数据集没有
  - target_names: 目标(标签) 名称, 列表格式
```python
'''
  鸢尾花数据集练习
'''

from sklearn.datasets import load_iris


def datasets_demo():
  """
    sklearn 数据集使用
    :return:
  """

  # 获取鸢尾花数据集
  iris = load_iris()
  print("鸢尾花数据集的返回值：\n", iris)

  # 返回值是一个继承自字典的Bench
  print("鸢尾花的特征值：\n", iris["data"], iris.data.shape)
  print("鸢尾花的目标值: \n", iris.target)
  print("查看数据集描述：\n", iris["DESCR"])
  print("查看特征名：\n", iris.feature_names)

if __name__ == '__main__':
  datasets_demo()
```
#### 5.数据集的划分
机器学习一般的数据集会划分为两个部分：
- 训练数据：用于训练，构建模型
- 测试数据：在模型检验时使用，用于评估模型是否有效

划分比例：
- 训练集：70% ~ 80% ~ 75%
- 测试集：30% ~ 20% ~ 25%

##### 数据集划分 api
- sklearn.model_selection.train_test_split(arrays, *options)
  - x 数据集的特征值
  - y 数据集的标签值
  - test_size 测试集的大小，一般为float
  - random_state 随机数种子，不同的种子会造成不同的随机采样结果。相同的种子采样结果相同
  - return 训练集特征值(x_train)，测试集特征值(x_test)，训练集目标值(y_train)，测试集目标值(y_test)

```python
'''
  鸢尾花数据集练习
'''

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

def datasets_demo():
  """
    sklearn 数据集使用
    :return:
  """

  # 获取鸢尾花数据集
  iris = load_iris()

  # 数据集划分
  # 特征值，目标值，测试集比例，随机数种子
  x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size = 0.2, random_state = 22)
  print("训练集特征值：\n", x_train, x_train.shape)

if __name__ == '__main__':
  datasets_demo()
```

## 3. 特征工程介绍
数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。
### 3.1 什么是特征工程
特征工程是使用专业背景知识和技巧处理数据，使得特征能在机器学习算法上发挥更好的作用的过程。
- 意义：会直接影响机器学习的效果。
### 3.2 特征工程的位置与数据处理的比较
- pandas: 一个数据读取方便以及基本的处理格式的工具
- sklean: 对于特征的处理提供了强大的接口
### 3.3 特征工程包含的内容
#### 3.3.1 特征抽取
将任意数据(如文本或图像)转换为可用于机器学习的数字特征
- 字典特征提取(特征离散化)
- 文本特征提取
- 图像特征提取(深度学习将介绍)
##### 1. 特征提取API
```python
sklearn.feature_extraction 
```

##### 2. 字典特征提取
> 作用：对字典数据进行特征值化
- sklearn.feature_extraction.DictVectorize(sparse=True,...)
  - DictVectorizer.fit_transform(X) X：字典或者包含字典的迭代器返回值：返回sparse矩阵。
  - DictVectorizer.inverse_transform(X) X: array 数组或者sparse矩阵 返回值：转换之前数据格式
  - DictVectorizer.get_feature_names() 返回类别名称
> 应用场景：数据集当中类别特征比较多
  1. 将数据集的特征 =》 字典类型
  2. DictVectorizer 转换
  3. 本身是字典类型数据
```python
'''
  字典特征提取
'''
from sklearn import feature_extraction
from sklearn.feature_extraction import DictVectorizer

def dict_vectorizer():
  data = [
    {'city': '北京', 'temperature': 100},
    {'city': '上海', 'temperature': 60},
    {'city': '深圳', 'temperature': 30},
  ]
  print("原始数据：\n", data)
  
  # 1. 实例化一个转换器类
  # transfer = DictVectorizer()
  transfer = DictVectorizer(sparse=False)
  
  # 2. 调用fit_transform()
  data_new = transfer.fit_transform(data)
  print(data_new)
  print("特征名称\n", transfer.get_feature_names_out())


if __name__ == '__main__':
  dict_vectorizer()

# 原始数据：
#  [{'city': '北京', 'temperature': 100}, {'city': '上海', 'temperature': 60}, {'city': '深圳', 'temperature': 30}]
# 
# [[  0.   1.   0. 100.]
#  [  1.   0.   0.  60.]
#  [  0.   0.   1.  30.]]
# 
# 特征名称
#  ['city=上海' 'city=北京' 'city=深圳' 'temperature']
#
#
# 解析：
# [[  0.   1.   0. 100.]
#  [  1.   0.   0.  60.]
#  [  0.   0.   1.  30.]]
# 纵向观察这个数组，
# 第0列表示是否是上海，如果是上海设置为1，否则为0；
# 第1列表示是否是北京，如果是北京设置为1，否则为0；
# 第2列表示是否是深圳，如果是深圳设置为1，否则为0；
# 第3列表示温度，分别代表3个城市的温度。
```

##### 3. 文本特征提取
> 作用：对文本数据进行特征值化
- sklearn.feature_extraction.text.CountVectorizer(stop_words=[])
  - 返回词频矩阵,统计每个样本特征词出现的个数，stop_words: 停用词
- CountVectorizer.fit_transform(X) X: 文本或者包含文本字符串的可迭代对象 返回值：返回sparse矩阵

- CountVectorizer.inverse_transform(X): X: array 数组或者sparse矩阵 返回值：转换之前数据格式

- CountVectorizer_get_feature_names() 返回值，单词列表

- sklearn.feature_extraction.text.TfidfVectorizer

> 关键词：在某一个类别的文章中，出现的次数很多，但是在其他类别的文章当中出现很少

```python
'''
  英文文本提取
'''

from sklearn.feature_extraction.text import CountVectorizer

def text_extract():
  data = [
    "life is short, i like like python",
    "life is too long, i dislike python",
  ]

  # 1. 实例化一个转换器类
  transfer = CountVectorizer()
  # 2. 调用fit_transform方法
  data_new = transfer.fit_transform(data)

  print("data_new \n", data_new.toarray())
  print("特征名字:", transfer.get_feature_names_out())

  # data_new
  #  [
  #   [0 1 1 2 0 1 1 0],
  #   [1 1 1 0 1 1 0 1]
  #  ]
  # 特征名字: ['dislike' 'is' 'life' 'like' 'long' 'python' 'short' 'too']

  return None


if __name__ == "__main__":
  text_extract()
```

```python
"""
  中文文本提取
"""
from sklearn.feature_extraction.text import CountVectorizer
import jieba # 中文文本分词

def text_chinese_extract():
  """
    中文文本特征抽取: 手动分词
  """
  data = [
    "我 爱 北京 天安门",
    "天安门 上 太阳 升",
  ]

  # 1. 实例化一个转换器类
  transfer = CountVectorizer()
  # 2. 调用fit_transform方法
  data_new = transfer.fit_transform(data)

  print("data_new \n", data_new.toarray())
  print("特征名字:", transfer.get_feature_names_out())
  return None

def cut_word(text):
  """
    进行中文分词：我爱北京天安门 => 我 爱 北京 天安门
  """
  res = " ".join(list(jieba.cut(text)))
  
  return res


def text_chinese_extract2():
  """
    中文文本特征抽取
  """

  # 1. 将中文文本进行分词
  data = [
    "一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天",
    "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",
    "如果只用一种方式了解某事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。",
  ]

  data_new = []
  for sent in data:
    res = cut_word(sent)
    data_new.append(res)

  print("data_new \n", data_new)

  # 1. 实例化一个转换器类
  transfer = CountVectorizer()
  # 2. 调用fit_transform方法
  data_final = transfer.fit_transform(data_new)

  print("data_new \n", data_final.toarray())
  print("特征名字:", transfer.get_feature_names_out())
  return None

if __name__ == "__main__":
  # text_extract()
  text_chinese_extract2()
  # cut_word('我爱北京天安门')

```

##### 4.Tf-idf 文本特征提取
- TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为这些词或者短语具有很好的类别区分能力，适合用来分类。
- TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。
###### 公式
- 词频(term frequency, TF)：指的是某一个给定的词语在该文件中出现的频率
- 逆向文档频率(inverse document frequency, IDF) 是一个词语普遍重要性的度量。某一特定词语的idf, 可以由总文件数目除以包含该词语之文件的数目，再将得到的商取以10为底的对数得到
> tfidfi,j = tfi,j * idfi
最终得出的结果可以理解为重要程度。

```python
'''
TF-IDF - 重要程度
  两个词 "经济", "非常"
  1000篇文章 - 语料库
  100篇文章 - "非常"
  10篇文章 - "经济"
  两篇文章
    文章A(100词)：10 次"经济" TF-IDF: 0.2
      tf: 10 / 100 = 0.1
      idf: lg 1000 / 10 = 2
    文章B(100词)：10 次"非常" TF-IDF: 0.1
      tf: 10 / 100 = 0.1
      idf: lg 1000 / 100 = 1
    TF - 词频(term frequency, tf)
    IDF - 逆向文档频率(inverse document frequency, idf)
'''
```
###### API
- sklearn.feature_extraction.text.TfidfVectorizer(stop_words=None, ...)
  返回词的权重矩阵
    - TfidfVectorizer.fit_transform(X)
      - X: 文本或者包含文本字符串的可迭代对象
      - 返回值: 返回sparse矩阵

    - TfidfVectorizer.inverse_transform(X)
      - X: array数组或者sparse矩阵
      返回值：转换之前数据格式

    - TfidfVectorizer.gett_feature_names()
      返回值：单词列表

```python
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
import jieba # 中文文本分词


def cut_word(text):
  """
    进行中文分词：我爱北京天安门 => 我 爱 北京 天安门
  """
  res = " ".join(list(jieba.cut(text)))
  
  return res


def tfidf_demo():
  '''
    tfidf 方法进行文本特征抽取
  '''
  # 1. 将中文文本进行分词
  data = [
    "一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天",
    "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",
    "如果只用一种方式了解某事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。",
  ]

  data_new = []
  for sent in data:
    res = cut_word(sent)
    data_new.append(res)

  print("data_new \n", data_new)

  # 1. 实例化一个转换器类
  transfer = TfidfVectorizer()
  # 2. 调用fit_transform方法
  data_final = transfer.fit_transform(data_new)

  print("data_new \n", data_final.toarray())
  print("特征名字:", transfer.get_feature_names_out())
  return None

if __name__ == '__main__':
  tfidf_demo()
```
#### 3.3.2 特征预处理
通过一些转换函数将特征数据转换厉更加适合算法模型的特征数据过程
##### 数值型数据的无量纲化：
需要用到一些方法进行无量纲化，使不同规格的数据转换到同一规格。
  - 归一化
  - 标准化
##### 特征预处理 API
```python
sklearn.preprocessing
```
##### 为什么要进行归一化/标准化？
特征的`单位或者大小相关较大，或者某特征的方差相比其它的特征要大出几个数量级，容易影响(支配)目标结果`，使得一些算法无法学习到其它的特征

##### 归一化
通过对原始数据进行变换把数据映射到(默认为[0, 1])之间
- x‘ = (x - min) / (max - min)
- x'' = x' * (max - min) + min'
作用于每一表，max为一列的最大值，min为一列的最小值,那么X''为最终结果，minx, max分别为指定区间值默认max为1，min为0

###### API
- sklearn.preprocessing.MinMaxScaler(feature_range=(min, max))
  - MinMaxScaler.fit_transform(X) 
    - X： numpy array 格式的数据[n_samples, n_features]
  - 返回值：转换后的形状相同的array
```python
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
def minmax_demo():
  """
    归一化
  """
  # 1. 获取数据
  data = pd.read_csv('dating.txt')
  data = data.iloc[:, :3]
  print("data: \n", data)

  # 2. 实例化一个转换器类
  transfer = MinMaxScaler()
  # 3. 调用fit_transform
  data_new = transfer.fit_transform(data)
  print("归一化后的数据：\n", data_new)



if __name__ == "__main__":
  minmax_demo()
```
###### 总结
注意最大值最小值是变化的，另外，最大值与最小值非常容易受异常点影响，`所以这种方法鲁棒性较差，只适合传统精确小数据场景。`

##### 标准化
通过对原始数据进行变化把数据变换到均值为0，标准差为1范围内
- x' = (x - mean) / σ：作用于每一列，mean为平均值，σ为标准差
<!-- - x'' = x' * std + mean -->
- 对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然人发生改变
- 对于标准化来说：如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小。
###### API
- sklearn.preprocessing.StandardScaler()
  - 处理之后，对每列来说，所有数据都聚集在均值为0附近，标准差为1
  - StandardScaler.fit_transform(X)
    - X: numpy array 格式的数据[n_samples, n_features]
  - 返回值：转换后的形状相同的array
```python
from sklearn.preprocessing import MinMaxScaler, StandardScaler
import pandas as pd

def stand_demo():
  """
    标准化
  """
  # 1. 获取数据
  data = pd.read_csv('dating.txt')
  data = data.iloc[:, :3]
  print("data: \n", data)

  # 2. 实例化一个转换器类
  transfer = StandardScaler()
  # 3. 调用fit_transform
  data_new = transfer.fit_transform(data)
  print("标准化后的数据：\n", data_new)

if __name__ == "__main__":
  stand_demo()


```
###### 总结
在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。
#### 3.3.3 特征降维
`降维`是指在某些限定条件下，`降低随机变量(特征)个数`，得到`一组"不相关"主变量`的过程
- 降低随机变量的个数
- 相关特征(correlated feature)
> 正是因为在进行训练的时候，我们都是使用特征进行学习。如果特征本身存在问题或者特征之间相关性较强，对于算法学习预测会影响较大

##### 降维的两种方式
- 特征选择
- 主成分分析(可以理解一种特征提取的方式)

##### 特征选择
数据中包含冗余或相关变量(或称特征、属性、指标等)，旨在从原有特征中找出主要特征
###### 方法
- Filter(过滤式): 主要探究特征本身特点，特征与特征和目标值之间关联
  - 方差选择法：低方法特征过滤
  - 相关系数
- Embedded(嵌入式): 算法自动选择特征(特征与目标值之间的关联)
  - 决策树: 信息熵、信息增益
  - 正则化：L1, L2
  - 深度学习：卷积
```python
# sklearn.feature_selection
```
###### 低方差特征过滤
删除低方差的一些特征
- 特征方差小：某个特征大多样本的值比较相近
- 特征方差大：某个特征很多样本的值都有差别
- sklearn.feature_selection.VarianceThreshold(threshold = 0.0)
  - 删除所有低方差特征
  - Variance.fit_transform(X)
    - X: numpy array 格式的数据[n_samples, n_features]
    - 返回值：训练集差异低于threshold的特征将被删除。默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征。
- 相关系数
  - 皮尔逊相关系数(Pearson Correlation Coefficient)
    - 反映变量之间相关关系密切程度的统计指标
- 相关系数特点
  相关系数的值介于-1与 +1 之间，即 -1 <= r <= +1。其性质如下：
  - 当 r > 0 时，表示现变量正相关，r < 0 时，表示现变量负相关。
  - 当 |r| = 1时，表示两变量为完全相关，当 r = 0时，表示两变量间无相关关系。
  - 当 0 < |r| < 1时，表示两变量存在一定程度的相关。且|r|越接近1，两变量间线性关系越密切；|r|越接近0，两变量间线性关系越弱。
  - 一般可按三级划分L：|r| < 0.4 为低度相关；0.4 <= |r| < 0.7 为显著性相关；0.7 <= |r| <=1 为高度线性相关。
- from scipy.stats import pearsonr
  - x: (N, ) array_like
  - y: (N, ) array_list Returns:(Pearson's correlation coefficient, p-value)
- 特征与特征之间相关性很高：
  1. 选取其中一个
  2. 加权求和

```python
import pandas as pd 
from sklearn.feature_selection import VarianceThreshold
from scipy.stats import pearsonr

def variance_demo():
  """
    过滤低方差特征
  """
  # 获取数据
  data = pd.read_csv('factor_retures.csv')
  data = data.iloc[:, 1:-2]
  print("data: \n", data)
  # 实例化一个转换器类
  # transfer = VarianceThreshold()
  transfer = VarianceThreshold(threshold = 10)
  # 调用fit_transform
  data_new = transfer.fit_transform(data)
  print("data_new: \n", data_new, data_new.shape)

  # 计算某两个变量之间的相关系数
  r = pearsonr(data['pe_ratio'], data["pb_ratio"])
  print("相关系数: \n", r)

if __name__ == "__main__":
  variance_demo()
```
###### 主成分分析(PCA)
- 定义：高维数据转化为低维数据的过程，在此过程中可能会舍弃原有数据、创造新的变量
- 作用：是数据维数压缩，尽可能降低原数据的维数(复杂度)，损失少量数据精度，尽可能让信息损失最小
- 应用：回归分析或者聚类分析当中
- 找到一个合适的直线，通过一个矩阵运算得出主成分分析的结果(不需要理解)
- sklearn.decomposition.PCA(n_components=0.95)
  - 将数据分解为较低维数空间
  - n_components:
    - 小数：表示保留百分之多少的信息
    - 整数：减少到多少特征
  - PCA.fit_transform(X) X: numpy array 格式的数据
    [n_samples, n_features]
  - 返回值：转换后指定维度的array
```python
from sklearn.decomposition import PCA
def pca_demo():
  """
    PCA 降维
  """
  data = [
    [2, 8, 4, 5],
    [6, 3, 0, 8],
    [5, 4, 9, 1]
  ]
  # 1. 实例化一个转换器类
  # transfer = PCA(n_components=2)
  transfer = PCA(n_components=0.95)
  # 2. 调用fit_transform
  data_new = transfer.fit_transform(data)

  print("data_new \n", data_new)
  return None

if __name__ == "__main__":
  pca_demo()
```
##### instacart 降维案例
###### 数据如下(kaggle下载)
- order_products_prior.csv: 订单与商品信息
  - 字段：order_id, product_id, add_tocart_odrer,reordered
- products.csv: 商品信息
  - 字段：product_id, product_name, aisle_id, department_id
- orders.cvs: 用户的订单信息
  - 字段：order_id, user_id, eval_set, order_number,...
- aisies.cvs: 商品所属的具体物品类别 
  - 字段：aisle_id, aisle
```python
'''
  kaggle 下载 instacart 数据

  # 1. 获取数据

  # 2. 合并表
    - order_products_prior.csv: 订单与商品信息
    - 字段：order_id, product_id, add_tocart_odrer,reordered
    - products.csv: 商品信息
      - 字段：product_id, product_name, aisle_id, department_id
    - orders.cvs: 用户的订单信息
      - 字段：order_id, user_id, eval_set, order_number,...
    - aisies.cvs: 商品所属的具体物品类别 
      - 字段：aisle_id, aisle


  # 3. 找到 user_id 和 aisle 之间的关系

  # 4. PCA 降维
'''
import pandas as pd
from sklearn.decomposition import PCA

# 1. 获取数据
order_products = pd.read_csv("./instacart/order_products_prior.csv")
products = pr.read_csv("./instacart/products.csv")
orders = pd.read_csv("./instacart/orders.csv")
aisles = pd.read_csv("./instacart/aisles.csv")

# 2. 合并表
# 合并 aisles 和 products, aisle 和 product_id
tab1 = pd.merge(aisies, products, on = ["aisle_id", "aisle_id"])
tab2 = pd.merge(tab1, order_products, on = ["product_id", "product_id"])
tab3 = pd.merge(tab2, orders, on = ["order_id", "order_id"])

# 3. 找到 user_id 和 aisle 之间的关系
table = pd.crosstab(tab3["user_id"], tab3["aisle"])
# 取前10000数据
data = table [:10000]

# 4. PCA 降维
transfer = PCA(n_components = 0.95)
data_new = transfer.fit_transform(data)

print(data_new, data_new.shape)
```
